<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>KANUTE - behaviour testing between softwares and versions   <a name="Top"></a></title>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}

pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>KANUTE - behaviour testing between softwares and versions   <a name="Top"></a></h1>

<h1>Example: Kadmon</h1>

<h3>Dave T. Gerrard, University of Manchester</h3>

<h3>Produced 2014-07-08</h3>

<p><a href="http://personalpages.manchester.ac.uk/staff/David.Gerrard/">Dave&#39;s website</a></p>

<h2>Table of contents</h2>

<ul>
<li><a href="#Overview">Overview</a></li>
<li><a href="#test_dir">Tests directory</a></li>
<li><a href="#Example">Comparing tests example</a></li>
</ul>

<p><a href="#Top">Back to top</a></p>

<h1><a name="overview"></a>Overview</h1>

<p>This is a framework and piece of software to support:-</p>

<ul>
<li>checking if a specific behaviour of a piece of software is constant between versions.</li>
<li>checking if the behaviour of one piece of software is identical to the supposed same behaviour in another piece of software.</li>
</ul>

<p>It is <strong>not</strong> intended for performance benchmarking or for comparing <em>real world</em> noisy data.</p>

<p>The process has two parts:-</p>

<ol>
<li>A set of unit tests. These can be written in any language but must produce a pre-defined output. Tests and Outputs are organised in a structured testing folder/directory. Users are free to develop individual tests over a long period of time. Reports can then be run on an ad-hoc basis, using all or just a selection of test results.</li>
<li>Software to parse the tests, their output and make comparisons between versions and between softwares. The current implementation is in R (see below) but could be ported to any language.</li>
</ol>

<p><a href="#Top">Back to top</a></p>

<h1><a name="test_dir"></a>Tests directory</h1>

<p>The <a href="../">testing folder</a> will contain the following sub-folders</p>

<ul>
<li><a href="../inputs/">inputs</a>      a collection of minimal input data, used by tests to produce outputs</li>
<li><a href="../tests/">tests</a>        the individual test scripts, arranged by <strong>software</strong></li>
<li><a href="../outputs/">outputs</a>    the individual test results, arranged by <strong>software</strong> then <em>version</em></li>
<li><a href="../reports/">reports</a>    an optional directory containing reports (like this one)<br></li>
</ul>

<p>The software sub-folders within tests/ should contain a file called SOFTWARE.versions listing the version IDs. This is also a good place to list full paths to executable for each version, if so, the file should be tab delimited. Example: <a href="../tests/bowtie/bowtie.versions">bowtie.versions</a></p>

<p>The individual test scripts should specify the name of the output file they generate. Currently, the KANUTE package can detect these in bash scripts if they are assigned to the <em>TEST_OUT_NAME</em> variable.</p>

<p><a href="#Top">Back to top</a></p>

<h1><a name="Example"></a>Comparing tests example</h1>

<p>Running the tests is kept separate from comparing the results between different versions or software.</p>

<p>The test comparisons have initially been implemented in R and make use of the md5sum program.</p>

<h2>Starting the R session</h2>

<p>The user begins by giving the directory where the tests are stored and defining a subset of software names and version that they are interested in. The following workflow is in R </p>

<pre><code class="r">########### Reading testing results produced earlier (no execution, only comparison).
#
parent_dir &lt;-  paste(Sys.getenv(&quot;SCRATCH&quot;),&quot;kanute&quot;, sep=&quot;/&quot;)
testingFolder &lt;- paste(parent_dir, &quot;TestFolder&quot;, sep=&quot;/&quot;)
testDirTop &lt;- paste(testingFolder, &quot;tests&quot;,sep=&quot;/&quot;)
outputsDirTop &lt;- paste(testingFolder, &quot;outputs&quot;,sep=&quot;/&quot;)
source(paste(parent_dir, &quot;scripts/kanute_funcs.R&quot;, sep=&quot;/&quot;))
#
#
softwares &lt;- c(&quot;bowtie&quot;, &quot;bowtie2&quot;)
</code></pre>

<h2>Scan for tests in the testing folder</h2>

<p>There is a function <em>scanTestDir()</em> to got through the directory and pick out softwares, versions and tests. This produces a list object.</p>

<pre><code class="r">#
testing.list &lt;- scanTestDir(softwares=softwares, testDirTop=testDirTop)
testing.list
</code></pre>

<pre><code>## $bowtie
## $bowtie$outputs
## bowtie_singleRead_1.generic.test.sh bowtie_singleRead_2.generic.test.sh      bowtie_version.generic.test.sh 
##                  &quot;single_read1.sam&quot;                  &quot;single_read2.sam&quot;                       &quot;version.out&quot; 
## 
## $bowtie$versions.info
##       V1                                      V2
## 1 0.12.7 /opt/gridware/apps/bowtie/0.12.7/bowtie
## 2 0.12.8 /opt/gridware/apps/bowtie/0.12.8/bowtie
## 3 0.12.9 /opt/gridware/apps/bowtie/0.12.9/bowtie
## 4  1.0.0  /opt/gridware/apps/bowtie/1.0.0/bowtie
## 5  1.0.1  /opt/gridware/apps/bowtie/1.0.1/bowtie
## 
## 
## $bowtie2
## $bowtie2$outputs
## bowtie2_singleRead_1.generic.test.sh bowtie2_singleRead_2.generic.test.sh      bowtie2_version.generic.test.sh 
##                   &quot;single_read1.sam&quot;                   &quot;single_read2.sam&quot;                        &quot;version.out&quot; 
## 
## $bowtie2$versions.info
##            V1                                             V2
## 1       2.2.3       /opt/gridware/apps/bowtie2/2.2.3/bowtie2
## 2       2.2.2       /opt/gridware/apps/bowtie2/2.2.2/bowtie2
## 3       2.2.1       /opt/gridware/apps/bowtie2/2.2.1/bowtie2
## 4       2.2.0       /opt/gridware/apps/bowtie2/2.2.0/bowtie2
## 5       2.1.0       /opt/gridware/apps/bowtie2/2.1.0/bowtie2
## 6       2.0.5       /opt/gridware/apps/bowtie2/2.0.5/bowtie2
## 7       2.0.2       /opt/gridware/apps/bowtie2/2.0.2/bowtie2
## 8 2.0.0-beta7 /opt/gridware/apps/bowtie2/2.0.0-beta7/bowtie2
## 9 2.0.0-beta6 /opt/gridware/apps/bowtie2/2.0.0-beta6/bowtie2
</code></pre>

<pre><code class="r">#
</code></pre>

<h2>Compile md5sums for each test</h2>

<p>Using the result of <em>scanTestDir</em> the user can then get md5sum CHECKSUM values for every test with a result file. Again this produces another list.</p>

<pre><code class="r">#
test.library &lt;- compileTestResults(testing.list, outputsDirTop)
#test.library   # don&#39;t show this, it&#39;s a big hier-archichal list of md5sums
#
#
</code></pre>

<h1>Compare results across versions</h1>

<p>The above list has all the information to check whether any two tests produced the same output, or not. The default behaviour of <em>compareTests</em> is to check for concordance between versions <strong>within</strong> one piece of software.</p>

<pre><code class="r">test.results &lt;- compareTests(test.library)
</code></pre>

<pre><code>## [1] &quot;#######KANUTE###########: Comparing test results WITHIN each software&quot;
## [1] &quot;bowtie&quot;
## [1] &quot;Using version 0.12.7 as reference&quot;
## [1] &quot;Processing 5 versions...&quot;
## [1] &quot;0.12.7&quot;
## [1] &quot;0.12.8&quot;
## [1] &quot;0.12.9&quot;
## [1] &quot;1.0.0&quot;
## [1] &quot;1.0.1&quot;
## [1] &quot;bowtie2&quot;
## [1] &quot;Using version 2.2.3 as reference&quot;
## [1] &quot;Processing 9 versions...&quot;
## [1] &quot;2.2.3&quot;
## [1] &quot;2.2.2&quot;
## [1] &quot;2.2.1&quot;
## [1] &quot;2.2.0&quot;
## [1] &quot;2.1.0&quot;
## [1] &quot;2.0.5&quot;
## [1] &quot;2.0.2&quot;
## [1] &quot;2.0.0-beta7&quot;
## [1] &quot;2.0.0-beta6&quot;
</code></pre>

<pre><code class="r">test.results
</code></pre>

<pre><code>## $bowtie
##                                  test 0.12.7 0.12.8 0.12.9 1.0.0 1.0.1
## 1 bowtie_singleRead_1.generic.test.sh   TRUE   TRUE   TRUE  TRUE  TRUE
## 2 bowtie_singleRead_2.generic.test.sh   TRUE   TRUE   TRUE  TRUE  TRUE
## 3      bowtie_version.generic.test.sh   TRUE  FALSE  FALSE FALSE FALSE
## 
## $bowtie2
##                                   test 2.2.3 2.2.2 2.2.1 2.2.0 2.1.0 2.0.5 2.0.2 2.0.0-beta7 2.0.0-beta6
## 1 bowtie2_singleRead_1.generic.test.sh  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE        TRUE        TRUE
## 2 bowtie2_singleRead_2.generic.test.sh  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE        TRUE        TRUE
## 3      bowtie2_version.generic.test.sh  TRUE FALSE FALSE FALSE FALSE FALSE FALSE       FALSE       FALSE
</code></pre>

<pre><code class="r">#
</code></pre>

<h2>Compare results for some tests across softwares</h2>

<p>An important feature of KANUTE is to test for differences in results <strong>between</strong> different softwares. This depends on the user having written tests that produce comparable results. The user must then also provide a table of mappings between specific tests performed on two different softares. Here is an example <a href="../tests/test.mappings">file</a> </p>

<pre><code class="r">## created a tab-delim file called test.mappings in top of tests/ folder
## mapping.name software1 software1.testA software2   software2.testA TRUE
## the final column is TRUE if expect results to be the same, FALSE if not or NA if unknown.
## May need to be 1/0 instead of TRUE/FALSE
#
test.mappings &lt;- read.delim(paste(testDirTop, &quot;test.mappings&quot;, sep=&quot;/&quot;), header=F, stringsAsFactors=F)
names(test.mappings) &lt;- c(&quot;mapping.name&quot;, &quot;software1&quot;, &quot;software1.test&quot;, &quot;software2&quot;, &quot;software2.test&quot;, &quot;expectIdentical&quot;)
test.mappings
</code></pre>

<pre><code>##           mapping.name software1                      software1.test software2                       software2.test expectIdentical
## 1 singleRead_1.generic    bowtie bowtie_singleRead_1.generic.test.sh   bowtie2 bowtie2_singleRead_1.generic.test.sh           FALSE
## 2 singleRead_2.generic    bowtie bowtie_singleRead_2.generic.test.sh   bowtie2 bowtie2_singleRead_2.generic.test.sh            TRUE
</code></pre>

<p>Then, giving this <em>data.frame</em> to <em>compareTests</em> will make a comparison BETWEEN softwares.</p>

<pre><code class="r">test.results &lt;- compareTests(test.library, mapped.tests=test.mappings)
</code></pre>

<pre><code>## [1] &quot;#######KANUTE###########: Comparing tests BETWEEN softwares&quot;
## [1] &quot;2 mapped tests&quot;
</code></pre>

<pre><code class="r">test.results
</code></pre>

<pre><code>## $MAPPED.TESTS
##           mapping.name software1                      software1.test software2                       software2.test expectIdentical result   ver1  ver2
## 1 singleRead_1.generic    bowtie bowtie_singleRead_1.generic.test.sh   bowtie2 bowtie2_singleRead_1.generic.test.sh           FALSE  FALSE 0.12.7 2.2.3
## 2 singleRead_2.generic    bowtie bowtie_singleRead_2.generic.test.sh   bowtie2 bowtie2_singleRead_2.generic.test.sh            TRUE   TRUE 0.12.7 2.2.3
</code></pre>

<pre><code class="r">### specify the reference versions to be used in tests BETWEEN software
test.results &lt;- compareTests(test.library, mapped.tests=test.mappings, ref.versions=list(bowtie=&quot;1.0.1&quot;, bowtie2=&quot;2.2.3&quot;))
</code></pre>

<pre><code>## [1] &quot;#######KANUTE###########: Comparing tests BETWEEN softwares&quot;
## [1] &quot;2 mapped tests&quot;
</code></pre>

<pre><code class="r">test.results
</code></pre>

<pre><code>## $MAPPED.TESTS
##           mapping.name software1                      software1.test software2                       software2.test expectIdentical result  ver1  ver2
## 1 singleRead_1.generic    bowtie bowtie_singleRead_1.generic.test.sh   bowtie2 bowtie2_singleRead_1.generic.test.sh           FALSE  FALSE 1.0.1 2.2.3
## 2 singleRead_2.generic    bowtie bowtie_singleRead_2.generic.test.sh   bowtie2 bowtie2_singleRead_2.generic.test.sh            TRUE   TRUE 1.0.1 2.2.3
</code></pre>

<p>By default, if mapped.tests parameter given, tests <strong>within</strong> Software are suppressed. Here&#39;s how to turn it back on</p>

<pre><code class="r"># Here&#39;s how to turn it back on
test.results &lt;- compareTests(test.library, mapped.tests=test.mappings, show.all.tests=TRUE)
</code></pre>

<pre><code>## [1] &quot;#######KANUTE###########: Comparing test results WITHIN each software&quot;
## [1] &quot;bowtie&quot;
## [1] &quot;Using version 0.12.7 as reference&quot;
## [1] &quot;Processing 5 versions...&quot;
## [1] &quot;0.12.7&quot;
## [1] &quot;0.12.8&quot;
## [1] &quot;0.12.9&quot;
## [1] &quot;1.0.0&quot;
## [1] &quot;1.0.1&quot;
## [1] &quot;bowtie2&quot;
## [1] &quot;Using version 2.2.3 as reference&quot;
## [1] &quot;Processing 9 versions...&quot;
## [1] &quot;2.2.3&quot;
## [1] &quot;2.2.2&quot;
## [1] &quot;2.2.1&quot;
## [1] &quot;2.2.0&quot;
## [1] &quot;2.1.0&quot;
## [1] &quot;2.0.5&quot;
## [1] &quot;2.0.2&quot;
## [1] &quot;2.0.0-beta7&quot;
## [1] &quot;2.0.0-beta6&quot;
## [1] &quot;#######KANUTE###########: Comparing tests BETWEEN softwares&quot;
## [1] &quot;2 mapped tests&quot;
</code></pre>

<pre><code class="r">#What have we got at the end of all that
test.results
</code></pre>

<pre><code>## $bowtie
##                                  test 0.12.7 0.12.8 0.12.9 1.0.0 1.0.1
## 1 bowtie_singleRead_1.generic.test.sh   TRUE   TRUE   TRUE  TRUE  TRUE
## 2 bowtie_singleRead_2.generic.test.sh   TRUE   TRUE   TRUE  TRUE  TRUE
## 3      bowtie_version.generic.test.sh   TRUE  FALSE  FALSE FALSE FALSE
## 
## $bowtie2
##                                   test 2.2.3 2.2.2 2.2.1 2.2.0 2.1.0 2.0.5 2.0.2 2.0.0-beta7 2.0.0-beta6
## 1 bowtie2_singleRead_1.generic.test.sh  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE        TRUE        TRUE
## 2 bowtie2_singleRead_2.generic.test.sh  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE        TRUE        TRUE
## 3      bowtie2_version.generic.test.sh  TRUE FALSE FALSE FALSE FALSE FALSE FALSE       FALSE       FALSE
## 
## $MAPPED.TESTS
##           mapping.name software1                      software1.test software2                       software2.test expectIdentical result   ver1  ver2
## 1 singleRead_1.generic    bowtie bowtie_singleRead_1.generic.test.sh   bowtie2 bowtie2_singleRead_1.generic.test.sh           FALSE  FALSE 0.12.7 2.2.3
## 2 singleRead_2.generic    bowtie bowtie_singleRead_2.generic.test.sh   bowtie2 bowtie2_singleRead_2.generic.test.sh            TRUE   TRUE 0.12.7 2.2.3
</code></pre>

<h2><a name="todo"></a>TO DO</h2>

<ul>
<li>Build in more tests for more software</li>
<li>Visualisation of results (Coloured grid, use Shiny)</li>
<li>Run/update tests from within R session</li>
<li>Allow for non-generic tests.</li>
<li>Tests between installations or compare against an archive (may need extra structure or second testingFolder).</li>
</ul>

<p><a href="#Top">Back to top</a></p>

<p><a href="#Top">Back to top</a></p>

<h2><a name="Anchor_name"></a>Start of next section</h2>

<p><a href="#Top">Back to top</a></p>

<h2>About this file</h2>

<p>Produced from an R-markdown (Rmd) file ../reports/example.Rmd . </p>

<p><a href="#Top">Back to top</a></p>

<p>Information about the R session:-</p>

<pre><code class="r">sessionInfo()
</code></pre>

<pre><code>## R version 3.0.2 (2013-09-25)
## Platform: x86_64-redhat-linux-gnu (64-bit)
## 
## locale:
##  [1] LC_CTYPE=en_GB       LC_NUMERIC=C         LC_TIME=en_GB        LC_COLLATE=en_GB     LC_MONETARY=en_GB    LC_MESSAGES=en_GB    LC_PAPER=en_GB       LC_NAME=C            LC_ADDRESS=C         LC_TELEPHONE=C       LC_MEASUREMENT=en_GB LC_IDENTIFICATION=C 
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
## [1] knitr_1.6    markdown_0.7
## 
## loaded via a namespace (and not attached):
## [1] evaluate_0.5.5 formatR_0.10   stringr_0.6.2  tools_3.0.2
</code></pre>

</body>

</html>
